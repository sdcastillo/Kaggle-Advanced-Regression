---
title: "Kaggle - Advanced Regression for Housing Prices"
author: "Sam Castillo"
date: "November 13, 2018"
output: 
  html_document:
    toc: true
---


This notebook is borrowing heavily from the following kaggle submissions:

* 1.
* 2. 

#Executive Summary

Several regression techniques are used to predict housing prices.  I go very quickly through the exploratory analysis because I can just read the other EDA kernals on Kaggle in order to understand the data.  I spent most of my time around the model fitting process.

#Exploratory Analysis

We look at histograms of the numeric features.  Different types of models work well for different types of data, so before thinking about model selection we to understand the shape of the feature space.  We start by loading a series of packages and the data source.

```{r}
library(tidyverse)
packages <- c("gbm", "xgboost", "caret", "tidyr", "ggplot2", "lubridate", "corrplot", "caretEnsemble", "e1071", "ggridges", "forcats", "car", "fastDummies", "glmnet", "ggpubr", "xgboost", "broom")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)
```

```{r message = F}
list.files()
raw_train = read_csv("train.csv")
raw_test = read_csv("test.csv")
```

This is a wide data set, with about 80.  There are 43 categorical features and 38 numeric features.

```{r}
dim(raw_train)
raw_train %>% 
  map_dfr(class) %>% 
  gather(feature, type) %>% 
  group_by(type) %>% 
  summarise(n = n())
```


The target competition is the sale price of each home, and so pay special attention to this distribution.

```{r}
p1 <- raw_train %>% 
  ggplot(aes(SalePrice)) + 
  geom_histogram() + 
  ggtitle("Distribution of Sale Price")

p2 <- raw_train %>% 
  ggplot(aes(sample = SalePrice)) + 
  stat_qq() + 
  stat_qq_line() + 
  ggtitle("Empirical Normal Quantiles vs Theoretical Quantiles of Sale Price")

ggarrange(p1, p2, nrow = 1)
```

To make linear modeling easier, we take the log transform in order to make this distribution approximately normal.

```{r}
p1 <- raw_train %>% 
  mutate(SalePrice = log(SalePrice + 1)) %>% 
  ggplot(aes(SalePrice)) + 
  geom_histogram() + 
  ggtitle("Distribution of Log of Sale Price")

p2 <- raw_train %>% 
  mutate(SalePrice = log(SalePrice + 1)) %>% 
  ggplot(aes(sample = SalePrice)) + 
  stat_qq() + 
  stat_qq_line() + 
  ggtitle("Empirical Normal Quantiles vs Theoretical Quantiles of Log of Sale Price")

ggarrange(p1, p2, nrow = 1)
```

The numeric features are all measuremnets of the house.  For example, these are the total square footage, the number of bathrooms, the number of bedrooms, and so fourth.  We look at the distributions of these features with many histograms.

```{r}
numeric_cols <- raw_train %>% select(`1stFlrSF`, `2ndFlrSF`,YrSold, TotalBsmtSF, YearBuilt) %>% names()

raw_train %>% 
  select(numeric_cols) %>% 
  gather(column, value, 1:length(numeric_cols)) %>% 
  ggplot(aes(value)) + 
  geom_histogram() + 
  facet_wrap(vars(column), scales = "free")
```

Many of these columns are assymetric, which is another word for being skewed.

##Correcting Skewness

Linear models work best when the input feaures are symmetric.  We can rank each feature based on skewness.

```{r}
skewed_features <- raw_train %>% 
  select(-SalePrice) %>% #ignore the encoded features because these are not "real" numeric features
  select_if(is.numeric) %>% 
  summarise_all(skewness) %>% 
  gather(column, skewness) %>% 
  arrange(desc(skewness)) %>%
  filter(abs(skewness) > 0.75) # 0.75 value from kaggle kernal.  

skewed_features
```

We can look at the histograms for these features.

```{r}
raw_train %>% 
  select(skewed_features$column) %>% 
  mutate_all(scale) %>%  #this doesn't impact skewness
  gather("column", "x_value") %>% 
  left_join(skewed_features, by = "column") %>% 
  mutate(column = as.factor(column),
         column = fct_relevel(column, skewed_features$column)) %>% 
  ggplot(aes(x_value, column)) + 
  geom_density_ridges() + 
  xlim(-2, 2) + 
  ggtitle("Distributions of Skewed Features")
```

We fix skewness with a box cox transform

```{r}
#apply box cox transform of 1 + x for each of the skewed columns
apply_box_cox <- function(feature) {bcPower(feature + 1, lambda =  0.15)}

model_data <- raw_train %>% 
  mutate_at(skewed_features$column, apply_box_cox)

raw_train %>% 
  dplyr::select(skewed_features$column) %>% 
  mutate_all(apply_box_cox) %>% #this should fix skewness
  # mutate_all(scale) %>%  #this doesn't impact skewness
  gather("x_name", "x_value") %>% 
  ggplot(aes(x_value, x_name)) + 
  geom_density_ridges() + 
  xlim(-0.8, 2) + 
  ggtitle("Distributions of Skewed Features After Box Cox Transform")
```

We see that the transformation greatly improves the skewness.  This is not perfect as `kitchenAbvGr` is still slightly skewed, but this is a big improvement.

##Label Encoding

Many of the factors in this data have an implied ranking, but are out of order by default.  For example, suppose that there was a measure for the overall quality of the house from Low to High.  The proper order of this would be (Low, Mid, High) which can be encoded as (1, 2, 3).  In this data set, this feature would be out of order (Mid, High, Low), for example.

These features need to be put into the proper order.


 [1] "MSSubClass"   "Street"       "Alley"        "LotShape"     "LandSlope"    "OverallCond"  "ExterQual"   
 [8] "ExterCond"    "BsmtQual"     "BsmtCond"     "BsmtExposure" "BsmtFinType1" "BsmtFinType2" "HeatingQC"   
[15] "CentralAir"   "KitchenQual"  "Functional"   "FireplaceQu"  "GarageFinish" "GarageQual"   "GarageCond"  
[22] "PavedDrive"   "PoolQC"       "Fence"        "MoSold"       "YrSold"     

Take for example the quality of the fireplace,`FireplaceQu`.  What is the ordering for these levels?

```{r}
raw_train$FireplaceQu %>% unique() %>% sort()
```

This order should actually be from worst to best: None, Poor (poor), Fa, TA, Gd, Ex.  We can easily see the improve correlation with the `SalePrice`.

```{r}
qualLevels <- c("None", "Po", "Fa", "TA", "Gd", "Ex")

before_releveling <- raw_train %>% 
  ggplot(aes(GarageQual, SalePrice)) + 
  geom_boxplot() + 
  ggtitle("Before Releveling")

after_releveling <- raw_train %>% transmute(SalePrice = SalePrice, 
                         GarageQual = fct_relevel(GarageQual, qualLevels)) %>% 
  ggplot(aes(GarageQual, SalePrice)) + 
  geom_boxplot() + 
  ggtitle("After Reveling")

ggarrange(before_releveling, after_releveling)
```

I perform similar steps on each of the qualitative variables, such as 'FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', and others.

##Dealing with Missing Values

There are a lot of missing values.

```{r}
sum(is.na(raw_train))
```

These can be seperated into two types 1) those with an actual NA, and 2) those with a blank.  These are dealt with by filling in the logical value.  For instance, a single story home will have 0 for 2nd floor area.  

```{r}
pct_na <- raw_train %>% 
  map_df(~sum(is.na(.x))/nrow(raw_train)) %>% 
  t()

#ignore features with na for now
miss_summary <- data_frame("feature" = rownames(pct_na), pct_na = as.vector(pct_na)) %>% arrange(pct_na)

miss_summary %>% 
  mutate(feature = fct_relevel(feature, miss_summary %>% select(feature) %>% unlist() %>% as.character())) %>% 
  filter(pct_na > 0) %>% 
  ggplot(aes(feature, pct_na)) + 
  geom_bar(stat = "identity") + 
  coord_flip() + 
  ggtitle("Percentage of Missing Values")
```

We impute them by proceeding sequentially through features with missing values. 

##Dealing with Outliers

There are at least two observations which do not make sense.  These are likely coding errors in the data or an improper record of the actual sale price of the home.  We can see that these two homes have a huge living area but sell for much less than \$400,000.  We drop these from the training data.

#Outliers

```{r}
raw_train %>% 
  mutate(outlier = ifelse(GrLivArea > 4000 & SalePrice < 4e+05, "outlier", "not_outlier")) %>% 
  ggplot(aes(GrLivArea, SalePrice, color = outlier)) + 
  geom_point() + 
  ggtitle("Two Outlying House Sales")
```

#Building a Model Dataset

In this section, I implement all of the changes which were shown previously in building a dataset for modeling.  This just allows me to have only one place to reference when I need to make changes as a result of the model output.  This is a lot of code in a short amount of space, but is just repeating the steps above on the complete data.

```{r}
#Get median LotFrontage by neighborhood for NA calculation below
LotFrontage_neighborhood_median <- raw_train %>% group_by(Neighborhood) %>% summarise(LotFrontage  = median(LotFrontage, na.rm = TRUE))

get_median <- function(row){
  cur_neighborhood = row %>% select(Neighborhood) %>% unlist() %>% as.character()
  cur_neighborhood
}

combined <- raw_train %>%
  modify_if(is.integer, as.double) %>% 
  mutate(train_source = "train") %>%
  filter(!(GrLivArea > 4000 & SalePrice < 4e+05)) %>% #remove two outlying points from training set.  Keep outliers in the test set because this is what Kaggle expects
  filter(!(OverallQual < 5 & SalePrice > 200000)) %>% #remove one additional outlier
  rbind(., (raw_test %>% mutate(SalePrice = NA, train_source = "test"))) %>% 
  mutate(MSSubClass = as.factor(MSSubClass),
         SalePrice = log(SalePrice + 1)) %>% #log transform sale price
  #Fill in Missing Values
  mutate_at(vars("PoolQC", "MiscFeature","Alley", "Fence", "GarageType", "GarageFinish", "GarageQual", "GarageCond", "FireplaceQu", "BsmtQual","BsmtExposure", "BsmtCond", "BsmtFinType1", "BsmtFinType2", "MasVnrType"), function(x){coalesce(x, "None")}) %>% 
  mutate_at(vars("GarageYrBlt", "GarageArea", "GarageCars", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "BsmtFullBath", "BsmtHalfBath", "MasVnrArea"), function(x){coalesce(x, 0)}) %>%
  mutate(MSZoning  = coalesce(MSZoning, "RL"),
         Functional = coalesce(Functional, "Typ"),
         Electrical  = coalesce(Electrical, "SBrkr"),
         KitchenQual = coalesce(KitchenQual, "TA"),
         Exterior1st  = coalesce(Exterior1st,"VinylSd"),
         Exterior2nd  = coalesce(Exterior2nd,"VinylSd"),
         SaleType  = coalesce(SaleType , "WD"),
         MSSubClass  = coalesce(MSSubClass , as.integer(0))) %>% #This means that there's no building class
  select(-Utilities) %>%  #only 1 unique level in training set.  Useless.
  left_join(LotFrontage_neighborhood_median, by = "Neighborhood") %>% #left join and fill in median for neighborhood
  mutate(LotFrontage = coalesce(LotFrontage.x, LotFrontage.y)) %>%  #fill in missing values with neighboorhood median
  select(-LotFrontage.y, -LotFrontage.x) %>% 
  modify_if(is.character, as.factor) %>% #convert characters to factors 
  mutate(TotalSF = TotalBsmtSF + `1stFlrSF` + `2ndFlrSF`)  #because square feet is an important predictor, we add a new feature which takes into account the total square feet of all floors
sum(is.na(combined)) == nrow(raw_test)

#relevel factors to be in a logical order
quality_vars <- c('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual')
order_quality_vars <- function(factor){droplevels(fct_relevel(factor, c("None", "Po", "Fa", "TA", "Gd", "Ex")))}

combined <- combined %>% mutate_at(quality_vars, order_quality_vars) %>% mutate_at(quality_vars, droplevels)

#check if factor levels match unique valeus
combined %>% select_if(is.factor) %>% map(function(x){as.character(setdiff(unique(x), levels(x)))}) %>% unique()

cols_to_encode <- c('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 
        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', 
        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',
        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', 
        'YrSold', 'MoSold')

# function which returns function which will encode vectors with values  of 'vec' 
label_encoder = function(vec){
  levels = sort(unique(vec))
  function(x){
    match(x, levels)
  }
}


for (column in cols_to_encode){
  current_data <- combined %>% select(column) %>% unlist()
  column_encoder <- label_encoder(current_data)
  combined[[column]] <- column_encoder(current_data)
}

#scale numeric features
#use the IQR to scale to make columns more rebust to outliers
scale_IQR <- function(x){
  IQR <- (quantile(x, 0.75) - quantile(x, 0.25))
  if(IQR == 0){
    (x - median(x))/sd(x)}
  else{
    (x - median(x))/IQR}
}

#apply IRQ scale
cols_to_scale <- combined %>% select_if(is.numeric) %>% select(-SalePrice, -Id) %>% names()

combined <- combined %>% 
  mutate_at(cols_to_scale, scale_IQR) 

#create dummy variables for factors
factor_columns <- combined %>% select_if(is.factor) %>% names()
combined <- combined %>% dummy_cols(remove_first_dummy = TRUE) %>% select(-factor_columns)
#check
sum(is.na(combined)) == nrow(raw_test)
```

##Near Zero Variables

One last change came as a result of failing to fit linear models.  Some factor levels have very low variance, and so the model matrix was sparse.  The quick solution is to drop these features.  The code below cuts off the variables which have 300 of the most common value for every 1 of the second most common value.

```{r}
near_zero_features <- nearZeroVar(combined, freqCut = 95/10)
combined <- combined %>% select(-near_zero_features)
```

Finally, after the data processing is complete, we recreate the training and test sets.  Kaggle expects the test set to still have an `Id` column.  This can be removed from the train data set but needs to remain in the test set.

```{r}
train <- combined %>% select(SalePrice, everything()) %>% filter(train_source_test == 0) %>% select(-Id, -train_source_test) %>% as.matrix()
train_x <- train[,-1]
train_y <- train[,1]
test <- combined %>%  filter(train_source_test == 1) %>%  select(-train_source_test) %>% as.matrix()
```




#Base Models

This is where I spent most of my time.  My process was to first train the model on the training data, make selections for parameters for each model, create an ensemble by combining them together using averages, and then submit these to Kaggle to get a test score.  The idea is to make a final prediction which is robust to outliers while capturing as much of the variation as possible by combining biased linear models with high-variance models (ie, GBMs).  This is what the most successful kaggle submissions have done.

##Baseline Linear Model

First, fit a baseline model using only a few features and define a function to measure the fit.  The metric we are using is the root mean squared error (RMSE).  We create a helper function to get the root mean squared error of the prediction against the training set.

```{r}
#I don't think this is working for the GLM net 
get_rmse <- function(input_model) {
  if(input_model$method == "glmnet"){
    #this method works by using the test data in the train object
    model_prediction <- predict(input_model, newx = test[,-1], s = input_model$bestTune$alpha)
  } else {
    model_prediction <- predict.train(input_model, train)
  }
  postResample(pred = model_prediction, obs = train[,"SalePrice"]) 
}
```

The baseline is a linear model with covariates `LotArea` and `OverallQual`, which are highly correlated with the sale price.

```{r}
regressControl  <- trainControl(method="repeatedcv",
                    number = 5,
                    repeats = 1, #set this to 1 for now
                    returnResamp = "all"
                    ) 

baseline <- train(SalePrice ~ LotArea + OverallQual,
           data = train,
           method  = "lm",
           trControl = regressControl)

get_rmse(baseline)
```

The RMSE was 0.21 after doing all of the feature engineering and ETL steps, which was a sign that I was doing something right.

##Regularized Regression: Lasso

Next we fit a Lasso.  This is just a GLM with a regularization component which automatically selects features for you by imposing a penalty of the total absolute value of the beta parameters.  

```{r}
lassoCtrl <- trainControl(method="repeatedcv", number=10, repeats = 3, verboseIter = F)
lassoGrid <-  expand.grid(alpha = 1, lambda = seq(0.001, 0.2, by = 0.005)) 

lasso <- train(x = train_x,
             y = train_y,
             method = "glmnet", 
             trControl = lassoCtrl,
             metric = "RMSE",
             tuneGrid = lassoGrid,
             preProcess = c("center", "scale")
             )

get_rmse(lasso)
```
The RMSE is quite low, which is a great sign.

Notes: 

1. Before removing near zero features the lasso RMSE was 0.10880182.  Changing this lowered the RMSE to 0.09776819

2. Because lasso is implemented in an old R library, the input x must be a model matrix with dummy variables.  This took me about 3 hours to figure out!

##Regularized Regression: Elastic Net

We can increase the flexibility of the Lasso by allowing alpha to vary.  This is also known as an Elastic Net model in the skikitlearn library in python.  When alpha is 1, this is the identical model as the Lasso.  When alpha is 0, the result is ridge regression.

```{r}
elasticNetCtrl <- trainControl(method="repeatedcv", number=10, repeats = 3, verboseIter = F)

elasticNetGrid <-  expand.grid(alpha = seq(0.1, 1, 0.03), lambda = seq(0.001, 0.2, by = 0.03)) 

elasticNet <- train(x = train_x,
             y = train_y,
             method = "glmnet", 
             trControl = elasticNetCtrl,
             metric = "RMSE",
             tuneGrid = elasticNetGrid
             )

get_rmse(elasticNet)
plot(elasticNet)
```

The alpha value is tending towards 1 on the cross-validation performance, which just makes the model the same as the lasso.

##Gradient Boosted Trees: Caret GBM

We fit a gradient boosted machine, which is a very powerful machine learning method.  This works by a concept know as boosting, which involves sequentially fitting regression trees on to the error terms from previous steps.

At iteration 1, the prediction is `y0` is just `f0(x)` = a constant.

At iteration 2, the prediction is `y1` = `f0(x)` + `f1(x)`

At iteration 3, the prediction is `y2` = `f0(x)` + `f1(x)` + `f2(x)`

etc.

The choice of `f` is usually a regression tree, and although very weak by itself, the predictions improve at each step and can be repeated up to thousands of times.  To learn more about GBMs in general, read (this excellent source)[https://speakerdeck.com/datasciencela/tianqi-chen-xgboost-overview-and-latest-news-la-meetup-talk] on a popular implementation known as xgboost.

```{r}
gbmCtrl <- trainControl(method="repeatedcv", number=5, repeats = 3, verboseIter = F)

gbmGrid <-  expand.grid(interaction.depth = c(1, 2, 3, 4), #most of the time this is sufficient
                    n.trees = c(1000, 2000), 
                    shrinkage = c(0.1, 0.05),
                    n.minobsinnode = c(25, 50)) 

gbm <- train(x = train_x,
             y = train_y,
             method = "gbm", 
             trControl = gbmCtrl,
             metric = "RMSE",
             tuneGrid = gbmGrid,
             train.fraction = 0.5
             )
plot(gbm)
```

```{r}
#this needs to be refit
saveRDS(gbm, "gbm.RDS")
# gbm <- readRDS(gbm, file = "gbm.RDS")
get_rmse(gbm)
```


```{r}
get_rmse(gbm)
var_importance <- varImp(gbm)$importance
data_frame(feature = rownames(var_importance), importance = var_importance$Overall) %>% 
  arrange(importance) %>% 
  mutate(feature = fct_inorder(feature)) %>% 
  top_n(50, wt = importance) %>% 
  ggplot(aes(feature, importance)) + 
  geom_bar(stat = "identity") + 
  coord_flip()  + 
  ggtitle("Top 50 most important features from GBM")
```

##Finely-Tuned GBM: XGBoost

Here we will spend a little extra time in tuning the parameters of this model.  This is closely following [this kaggle kernal](https://www.kaggle.com/pelkoja/visual-xgboost-tuning-with-caret).

The tuning parameters are

* nrounds: Number of trees, default: 100
* max_depth: Maximum tree depth, default: 6
* eta: Learning rate, default: 0.3
* gamma: Used for tuning of Regularization, default: 0
* colsample_bytree: Column sampling, default: 1
* min_child_weight: Minimum leaf weight, default: 1
* subsample: Row sampling, default: 1

We'll break down the tuning of these into five sections:

* Step 1. Fixing learning rate `eta` and number of iterations `nrounds`
* Step 2. Maximum depth `max_depth` and child weight `min_child_weight`
* Step 3. Setting column `colsample_bytree` and row sampling `subsample`
* Step 4. Experimenting with different `gamma` values
* Step 5. Reducing the learning rate `eta`

As a baseline to the xgboost, we'll first fit using the default parameters.

```{r}
grid_default <- expand.grid(
  nrounds = 100,
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

train_control <- caret::trainControl(
  method = "none",
  verboseIter = FALSE, # no training log
  allowParallel = TRUE # FALSE for reproducible results 
)

xgb_base <- caret::train(
  x = train_x,
  y = train_y,
  trControl = train_control,
  tuneGrid = grid_default,
  method = "xgbTree",
  verbose = TRUE
)
```

We'll start with the "bigger knobs" to tune and then use these settings to find the best of the "smaller knobs", and then come back and refine these more significant paramters.  We start by fixing the number of trees.  This controls the total number of regression trees to use.  This is selected in combination with the learning rate.  Using a lower learning rate updates the predictions more slowly and so requires a larger number of iterations, or `nrounds` in order to minimize the loss function.  Setting this too high eventually leads to instability.  Using more trees and a lower learning rate is almost always better, but has diminishing returns.  To start, in order to reduce compute time when choosing the other parameters, we set this to 1000.  After the other parameters have been chose, we will come back and turn this up.

```{r}
nrounds <- 1000
```

Then we can fill in the other items, using suggestions from (here)[https://www.slideshare.net/OwenZhang2/tips-for-data-science-competitions/14].  

```{r}
# note to start nrounds from 200, as smaller learning rates result in errors so
# big with lower starting points that they'll mess the scales
tune_grid <- expand.grid(
  nrounds = seq(from = 200, to = nrounds, by = 50),
  eta = c(0.025, 0.05, 0.1, 0.3),
  max_depth = c(2, 3, 4, 5, 6),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

tune_control <- caret::trainControl(
  method = "cv", # cross-validation
  number = 3, # with n folds 
  #index = createFolds(tr_treated$Id_clean), # fix the folds
  verboseIter = FALSE, # no training log
  allowParallel = TRUE # FALSE for reproducible results 
)

xgb_tune <- caret::train(
  x = train_x,
  y = train_y,
  trControl = tune_control,
  tuneGrid = tune_grid,
  method = "xgbTree",
  verbose = TRUE
)

# helper function for the plots
tuneplot <- function(x, probs = .90) {
  ggplot(x) +
    coord_cartesian(ylim = c(quantile(x$results$RMSE, probs = probs), min(x$results$RMSE))) +
    theme_bw()
}
```

From the plots above, we see that the best learning rate `eta` is at 0.05.  We will use this as the value moving forwards.  Next, we move on to finding a good value for the max tree depth.  We start with 3 +/- 1.  The maximum depth controls the depth or "height" of each tree and helps to avoid overfitting.  A higher depth can capture interaction effects better, but setting too high will overfit to the training set.

```{r}
tune_grid2 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = c(1, 2, 3, 4),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = c(1, 2, 3),
  subsample = 1
)

xgb_tune2 <- caret::train(
  x = train_x,
  y = train_y,
  trControl = tune_control,
  tuneGrid = tune_grid2,
  method = "xgbTree",
  verbose = TRUE
)

xgb_tune2$results %>% 
  ggplot(aes(nrounds, RMSE, color = as.factor(max_depth))) + 
  geom_line() + 
  ylim(0.125, 0.2) + 
  facet_wrap(vars(min_child_weight))
```


```{r}
xgb_tune2$bestTune
```

We see that the best max depth is 2 with  `min_child_wight` of 3.  The difference in RMSE is very small, indicating that our model is only making small improvements.

We continue to test the `colsample_bytree`, which is the same as the maximum number of features to be sampled for each tree, `max_features` from the GBM implementation.  Typical values are 0.5 - 1.

```{r}
tune_grid3 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = 0.05,
  max_depth = 2,
  gamma = 0,
  colsample_bytree = c(0.4, 0.6, 0.8, 1.0),
  min_child_weight = 3,
  subsample = c(0.5, 0.75, 1.0)
)

xgb_tune3 <- caret::train(
  x = train_x,
  y = train_y,
  trControl = tune_control,
  tuneGrid = tune_grid3,
  method = "xgbTree",
  verbose = TRUE
)

xgb_tune3$results %>% 
  ggplot(aes(nrounds, RMSE, color = as.factor(colsample_bytree))) + 
  geom_line() + 
  ylim(0.125, 0.135) + 
  facet_wrap(vars(subsample))
```

The best value for the fraction of rows to sample `subsample` is 1, that is, sample 100% of rows at each tree.  The best value of `colsample_bytree`, or the fraction of features to sample from in each tree, is 0.4, or 40%.

```{r}
xgb_tune3$bestTune
```


Now we can reduce the learning rate and use more trees.

```{r}
tune_grid5 <- expand.grid(
  nrounds = seq(from = 100, to = 10000, by = 200),
  eta = c(0.005, 0.01, 0.015, 0.025, 0.05),
  max_depth = 2,
  gamma = 0,
  colsample_bytree = 0.4,
  min_child_weight = 3,
  subsample = 1
)

xgb_tune5 <- caret::train(
  x = train_x,
  y = train_y,
  trControl = tune_control,
  tuneGrid = tune_grid5,
  method = "xgbTree",
  verbose = TRUE
)

tuneplot(xgb_tune5)
```

In theory, the RMSE should decrease as the number of iterations increases and the learning rate decreases.  The lowest RMSE on the training set is when `eta` is 0.05 and `nrounds` is 1000, but this is likely due to overfitting because it contradicts theory. This does not make sense and so I'm ignoring this result. The curves for `eta` of 0.01, 0.015, and 0.005 make a more sense because we see the RMSE decrease as the number of trees increases.  This implies that the best `eta` and `rnourds` values are at 0.01 and 2500.


```{r eval = F}
tune_grid_final <- expand.grid(
  nrounds = 2500,
  eta = 0.01,
  max_depth = 2,
  gamma = 0,
  colsample_bytree = 0.4,
  min_child_weight = 3,
  subsample = 1
)

xgb_tune_final <- caret::train(
  x = train_x,
  y = train_y,
  trControl = tune_control,
  tuneGrid = tune_grid5,
  method = "xgbTree",
  verbose = TRUE
)

get_rmse(xgb_tune_final)
```
The final RMSE is really low at 0.0887!  This model alone gave me better predictions than taking the average of the lasso, elastic net, and caret GBM.  To further improve our predictions we can build a meta-model using each of our four models, including the XGBoost.


#Stacking the Base Models

Define a function to take in a model and output predictions on the test set.  This should return a data frame with columns `Id`, `predicted sale price` (without a log).

#there should only be **one** copy of the data... This way each model can be evaluated based on this.  If there are multiple versions of the data, than each model needs to build its prediction on the data which it was trained on.

```{r}
make_test_prediction <- function(input_model){
  Id_temp <- test[,1] 
  data_to_test_temp <- test[, -1] #drop the Id column
   if(input_model$method == "glmnet asdlfkasdfl"){
    model_prediction <- predict(object = input_model$finalModel, newx = data_to_test_temp, s = input_model$bestTune$lambda) 
  } else {
    model_prediction <- predict.train(input_model, data_to_test_temp)
  }
  as.numeric(exp(model_prediction))
}
```

Try out our first predictions and submit to kaggle!

#Benchmark prediction is below 4000th
#first submission at 11/14/18 was 4000th on leaderboard.  This was using just a lasso

#XGB prediction

```{r}
data_frame(Id = as.integer(test[,1]), SalePrice = make_test_prediction(xgb_tune_final)) %>% 
  write_csv("XGBoost Predictions 20181117.csv")
```


The elastic net is fitting identical predictions as the lasso... great!

```{r}
data_frame(lasso = make_test_prediction(lasso), 
           elasticNet = make_test_prediction(elasticNet)) %>% 
  ggplot(aes(lasso, elasticNet)) + 
  geom_point()
```

```{r}
data_frame(Id = as.integer(test[,1]), SalePrice = make_test_prediction(gbm)) %>% 
  write_csv("GBM Predictions 20181114.csv")
```

Take averages of lasso and GBM prediction

```{r}

modelList <- list(lasso, elasticNet, gbm) #Need to create caretList of models usin gcaretEnsemble

# greedyEnsemble <- caretEnsemble(
# 
#   modelList, 
# 
#   metric="RMSE",
# 
#   trControl=trainControl(
# 
#     number=7, method = "cv"
# 
#   ))
```


```{r}
gbm_pred <- make_test_prediction(gbm)
lasso_pred <- make_test_prediction(lasso)
elasticNet_pred <- make_test_prediction(elasticNet)

final_pred <- (gbm_pred +lasso_pred +elasticNet_pred)/3

data_frame(Id = as.integer(test[,1]), SalePrice = final_pred) %>% 
  write_csv("GBM Lasso ElasticNet Straight Average Predictions 20181114.csv")

```

